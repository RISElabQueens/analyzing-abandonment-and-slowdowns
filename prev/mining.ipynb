from neo4j import GraphDatabase
import csv

# Neo4j connection details
uri = "bolt://localhost:7687"  
username = "neo4j"              
password = "12345678"          
# Initialize the driver without max_connection_lifetime
driver = GraphDatabase.driver(uri, auth=(username, password))

# Define the queries for each dataset
queries = {
    'artifacts': """
        MATCH (a:Artifact)
        RETURN
            a.id AS artifact_id,
            a.found AS artifact_found
    """,
    'releases': """
        MATCH (rel:Release)
        RETURN
            rel.id AS release_id,
            rel.version AS release_version,
            rel.timestamp AS release_timestamp
    """,
    'dependencies': """
        MATCH (rel:Release)-[d:dependency]->(depArtifact:Artifact)
        RETURN
            rel.id AS release_id,
            d.scope AS dependency_scope,
            d.targetVersion AS dependency_target_version,
            depArtifact.id AS dependency_artifact_id
    """,
    'added_values': """
        MATCH (rel:Release)-[:addedValues]->(av:AddedValue)
        RETURN
            rel.id AS release_id,
            av.id AS added_value_id,
            av.type AS added_value_type,
            av.value AS added_value
    """,
    'artifact_release': """
        MATCH (a:Artifact)-[:relationship_AR]->(rel:Release)
        RETURN
            a.id AS artifact_id,
            rel.id AS release_id
    """
}

def execute_and_stream(query_name, query):
    print(f"Executing query for {query_name}...")
    filename = f"{query_name}.csv"
    record_count = 0
    try:
        # Remove fetch_size from session creation
        with driver.session() as session:
            result = session.run(query)
            with open(filename, mode='w', newline='', encoding='utf-8') as csvfile:
                writer = None
                for record in result:
                    if writer is None:
                        # Initialize the CSV writer with headers
                        writer = csv.DictWriter(csvfile, fieldnames=record.keys())
                        writer.writeheader()
                    # Write the record to CSV
                    writer.writerow({key: record.get(key, '') for key in record.keys()})
                    record_count += 1
                    if record_count % 100000 == 0:
                        print(f"Processed {record_count} records for {query_name}...")
                print(f"Total records processed for {query_name}: {record_count}")
    except Exception as e:
        print(f"An error occurred while processing {query_name}: {e}")
    finally:
        print(f"Finished processing {query_name}.")

# Execute each query and stream the results
for query_name, query in queries.items():
    execute_and_stream(query_name, query)

# Close the driver
driver.close()
